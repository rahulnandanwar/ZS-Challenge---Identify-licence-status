{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".# Identify Licence Status (ZS Associates Case Study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to identify this problem "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Based on the training licence data we have to develop machine learning model that will correctly identify the licence status    in the test data.\n",
    "\n",
    "2) We have labelled training data having as below\n",
    "     Independent columns - ID, LICENSE ID, ACCOUNT NUMBER, SITE NUMBER, LEGAL NAME, DOING BUSINESS AS \n",
    "                           NAME, ADDRESS, CITY, STATE, ZIP, CODE, WARD, PRECINCT, WARD PRECINCT, POLICE DISTRICT, LICENSE                                  CODE, LICENSE DESCRIPTION, LICENSE NUMBER, APPLICATION TYPE, APPLICATION CREATED DATE, APPLICATION                              REQUIREMENTS COMPLETE, PAYMENT DATE, CONDITIONAL APPROVAL, LICENSE TERM START DATE, LICENSE TERM                                EXPIRATION DATE, LICENSE APPROVED FOR ISSUANCE, DATE ISSUED, LICENSE STATUS CHANGE DATE, SSA,                                  LATITUDE, LONGITUDE, LOCATION\n",
    "     Dependent(Target) column - LICENSE STATUS\n",
    "     \n",
    "3) Since we have labelled data for identification of LICENSE STATUS this problem can be solve by supervised learning approach.\n",
    "\n",
    "4) We have five distinct classes in the target column (AAI, AAC, REV, REA, INQ) so this is Multi-Class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages for model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Importing Train Dataset and Test Dataset\n",
    "\n",
    "dataset = pd.read_csv(\"train_file.csv\")\n",
    "testset = pd.read_csv(\"test_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the functions to perform missing value imputations and peform operations easily\n",
    "\n",
    "# let's create a variable to replace NA with the random sample imputation\n",
    "def impute_na(df, variable):\n",
    "    '''\n",
    "    This function is defined to perform random sampleing and then imputeing missing values from that samples.\n",
    "    '''\n",
    "    # random sampling\n",
    "    df[variable+'_random'] = df[variable]\n",
    "        \n",
    "    # extract the random sample to fill the na\n",
    "    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n",
    "    \n",
    "    # pandas needs to have the same index in order to merge datasets\n",
    "    random_sample.index = df[df[variable].isnull()].index\n",
    "    \n",
    "    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n",
    "    \n",
    "# let's create a variable to replace NA with the most frequent label\n",
    "def impute_na_most_frequent(df, variable):\n",
    "    '''\n",
    "    This function has defined to perform most frequent value impution for the missing values.\n",
    "    '''\n",
    "    most_frequent_category = df.groupby([variable])[variable].count().sort_values(ascending=False).index[0]\n",
    "    df[variable+ '_most_frequent'] = df[variable].fillna(most_frequent_category)\n",
    "    \n",
    "# let's create a variable to capture NA to indicate missingness\n",
    "def impute_na_missing(df, variable):\n",
    "    '''\n",
    "    This function has defined to mark the misssingness of that value in the dataset.\n",
    "    '''\n",
    "    # add additional variable to indicate missingness\n",
    "    df[variable+'_NA'] = np.where(df[variable].isnull(), 1, 0)\n",
    "    \n",
    "def impute_na_mean(df, variable):\n",
    "    '''\n",
    "    This function has defined to perform mean value imputation in the dataset.\n",
    "    '''\n",
    "    mean = df[variable].mean()\n",
    "    df[variable+'_mean'] = df[variable].fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_na(dataset,'ZIP CODE')\n",
    "impute_na(dataset,'APPLICATION REQUIREMENTS COMPLETE')\n",
    "impute_na(dataset,'LICENSE TERM START DATE')\n",
    "impute_na(dataset,'LICENSE TERM EXPIRATION DATE')\n",
    "impute_na(dataset,'LICENSE APPROVED FOR ISSUANCE')\n",
    "impute_na(dataset,'PAYMENT DATE')\n",
    "\n",
    "impute_na_most_frequent(dataset,'WARD PRECINCT')\n",
    "impute_na_most_frequent(dataset,'WARD')\n",
    "impute_na_most_frequent(dataset,'PRECINCT')\n",
    "impute_na_most_frequent(dataset,'POLICE DISTRICT')\n",
    "impute_na_most_frequent(dataset,'LICENSE STATUS CHANGE DATE')\n",
    "impute_na_most_frequent(dataset,'LATITUDE')\n",
    "impute_na_most_frequent(dataset,'LONGITUDE')\n",
    "impute_na_most_frequent(dataset,'DOING BUSINESS AS NAME')\n",
    "impute_na_most_frequent(dataset,'LICENSE NUMBER')\n",
    "\n",
    "impute_na_mean(dataset,'SSA')\n",
    "\n",
    "impute_na_missing(dataset,'WARD PRECINCT')\n",
    "impute_na_missing(dataset,'WARD')\n",
    "impute_na_missing(dataset,'PRECINCT')\n",
    "impute_na_missing(dataset,'POLICE DISTRICT')\n",
    "impute_na_missing(dataset,'SSA')\n",
    "impute_na_missing(dataset,'LICENSE STATUS CHANGE DATE')\n",
    "impute_na_missing(dataset,'LATITUDE')\n",
    "impute_na_missing(dataset,'LONGITUDE')\n",
    "\n",
    "dataset = dataset.drop(['ZIP CODE', 'APPLICATION REQUIREMENTS COMPLETE','LICENSE TERM START DATE',\n",
    "                        'LICENSE TERM EXPIRATION DATE','LICENSE APPROVED FOR ISSUANCE','APPLICATION CREATED DATE',\n",
    "                        'PAYMENT DATE','WARD PRECINCT','WARD','PRECINCT','POLICE DISTRICT','LICENSE STATUS CHANGE DATE',\n",
    "                        'LATITUDE','LONGITUDE','LOCATION','SSA','DOING BUSINESS AS NAME','LICENSE NUMBER'],axis=1)\n",
    "\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_na(testset,'ZIP CODE')\n",
    "impute_na(testset,'APPLICATION REQUIREMENTS COMPLETE')\n",
    "impute_na(testset,'LICENSE TERM START DATE')\n",
    "impute_na(testset,'LICENSE TERM EXPIRATION DATE')\n",
    "impute_na(testset,'LICENSE APPROVED FOR ISSUANCE')\n",
    "impute_na(testset,'PAYMENT DATE')\n",
    "\n",
    "impute_na_most_frequent(testset,'WARD PRECINCT')\n",
    "impute_na_most_frequent(testset,'WARD')\n",
    "impute_na_most_frequent(testset,'PRECINCT')\n",
    "impute_na_most_frequent(testset,'POLICE DISTRICT')\n",
    "impute_na_most_frequent(testset,'LICENSE STATUS CHANGE DATE')\n",
    "impute_na_most_frequent(testset,'LATITUDE')\n",
    "impute_na_most_frequent(testset,'LONGITUDE')\n",
    "impute_na_most_frequent(testset,'DOING BUSINESS AS NAME')\n",
    "impute_na_most_frequent(testset,'LICENSE NUMBER')\n",
    "\n",
    "impute_na_mean(testset,'SSA')\n",
    "\n",
    "impute_na_missing(testset,'WARD PRECINCT')\n",
    "impute_na_missing(testset,'WARD')\n",
    "impute_na_missing(testset,'PRECINCT')\n",
    "impute_na_missing(testset,'POLICE DISTRICT')\n",
    "impute_na_missing(testset,'SSA')\n",
    "impute_na_missing(testset,'LICENSE STATUS CHANGE DATE')\n",
    "impute_na_missing(testset,'LATITUDE')\n",
    "impute_na_missing(testset,'LONGITUDE')\n",
    "\n",
    "testset = testset.drop(['ZIP CODE', 'APPLICATION REQUIREMENTS COMPLETE','LICENSE TERM START DATE',\n",
    "                        'LICENSE TERM EXPIRATION DATE','LICENSE APPROVED FOR ISSUANCE','APPLICATION CREATED DATE',\n",
    "                        'PAYMENT DATE','WARD PRECINCT','WARD','PRECINCT','POLICE DISTRICT','LICENSE STATUS CHANGE DATE',\n",
    "                        'LATITUDE','LONGITUDE','LOCATION','SSA','DOING BUSINESS AS NAME','LICENSE NUMBER'],axis=1)\n",
    "# testset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handleing columns having Datetime information\n",
    "\n",
    "we need to extract the weekday, month and year from the respective date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_vars_with_day(df, var):\n",
    "    '''\n",
    "    Define a function to extract the Year, Month and day information from the datetime column features.\n",
    "    \n",
    "    '''\n",
    "    df[var +'_Year'] = pd.DatetimeIndex(df[var]).year\n",
    "        \n",
    "    df[var +'_Month'] = pd.DatetimeIndex(df[var]).month\n",
    "    \n",
    "    df[var +'_Day'] = pd.DatetimeIndex(df[var]).weekday\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vars_with_day(dataset,'DATE ISSUED')\n",
    "time_vars_with_day(dataset,'APPLICATION REQUIREMENTS COMPLETE_random')\n",
    "time_vars_with_day(dataset,'LICENSE TERM START DATE_random')\n",
    "time_vars_with_day(dataset,'LICENSE TERM EXPIRATION DATE_random')\n",
    "time_vars_with_day(dataset,'LICENSE APPROVED FOR ISSUANCE_random')\n",
    "time_vars_with_day(dataset,'PAYMENT DATE_random')\n",
    "time_vars_with_day(dataset,'LICENSE STATUS CHANGE DATE_most_frequent')\n",
    "\n",
    "dataset = dataset.drop(['DATE ISSUED','APPLICATION REQUIREMENTS COMPLETE_random','LICENSE TERM START DATE_random',\n",
    "             'LICENSE TERM EXPIRATION DATE_random','LICENSE APPROVED FOR ISSUANCE_random','PAYMENT DATE_random','LICENSE STATUS CHANGE DATE_most_frequent'], axis = 1)\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vars_with_day(testset,'DATE ISSUED')\n",
    "time_vars_with_day(testset,'APPLICATION REQUIREMENTS COMPLETE_random')\n",
    "time_vars_with_day(testset,'LICENSE TERM START DATE_random')\n",
    "time_vars_with_day(testset,'LICENSE TERM EXPIRATION DATE_random')\n",
    "time_vars_with_day(testset,'LICENSE APPROVED FOR ISSUANCE_random')\n",
    "time_vars_with_day(testset,'PAYMENT DATE_random')\n",
    "time_vars_with_day(testset,'LICENSE STATUS CHANGE DATE_most_frequent')\n",
    "\n",
    "testset = testset.drop(['DATE ISSUED','APPLICATION REQUIREMENTS COMPLETE_random','LICENSE TERM START DATE_random',\n",
    "             'LICENSE TERM EXPIRATION DATE_random','LICENSE APPROVED FOR ISSUANCE_random','PAYMENT DATE_random','LICENSE STATUS CHANGE DATE_most_frequent'], axis = 1)\n",
    "# testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function for frequency based encodeing\n",
    "\n",
    "def frequency_Encodeing(df,var):\n",
    "    '''\n",
    "    This function has defined to map the frequency based count in the dataset to perform encodeing.\n",
    "    '''\n",
    "    count_dict = df[var].value_counts().to_dict()\n",
    "    df[var + '_freq'] = df[var].map(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodeing on the dataset\n",
    "\n",
    "frequency_Encodeing(dataset,'LEGAL NAME')\n",
    "frequency_Encodeing(dataset,'SITE NUMBER')\n",
    "frequency_Encodeing(dataset,'DOING BUSINESS AS NAME_most_frequent')\n",
    "frequency_Encodeing(dataset,'ADDRESS')\n",
    "frequency_Encodeing(dataset,'CITY')\n",
    "frequency_Encodeing(dataset,'ZIP CODE_random')\n",
    "frequency_Encodeing(dataset,'STATE')\n",
    "frequency_Encodeing(dataset,'LICENSE CODE')\n",
    "frequency_Encodeing(dataset,'LICENSE NUMBER_most_frequent')\n",
    "frequency_Encodeing(dataset,'LICENSE DESCRIPTION')\n",
    "frequency_Encodeing(dataset,'WARD PRECINCT_most_frequent')\n",
    "\n",
    "#Encodeing on the testset\n",
    "frequency_Encodeing(testset,'LEGAL NAME')\n",
    "frequency_Encodeing(testset,'SITE NUMBER')\n",
    "frequency_Encodeing(testset,'DOING BUSINESS AS NAME_most_frequent')\n",
    "frequency_Encodeing(testset,'ADDRESS')\n",
    "frequency_Encodeing(testset,'CITY')\n",
    "frequency_Encodeing(testset,'ZIP CODE_random')\n",
    "frequency_Encodeing(testset,'STATE')\n",
    "frequency_Encodeing(testset,'LICENSE CODE')\n",
    "frequency_Encodeing(testset,'LICENSE NUMBER_most_frequent')\n",
    "frequency_Encodeing(testset,'LICENSE DESCRIPTION')\n",
    "frequency_Encodeing(testset,'WARD PRECINCT_most_frequent')\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.drop(['LEGAL NAME','SITE NUMBER','DOING BUSINESS AS NAME_most_frequent','ADDRESS','CITY','STATE','LICENSE CODE',\n",
    "                        'LICENSE NUMBER_most_frequent','LICENSE DESCRIPTION','WARD PRECINCT_most_frequent','ZIP CODE_random'], axis = 1)\n",
    "\n",
    "testset = testset.drop(['LEGAL NAME','SITE NUMBER','DOING BUSINESS AS NAME_most_frequent','ADDRESS','CITY','STATE','LICENSE CODE',\n",
    "                        'LICENSE NUMBER_most_frequent','LICENSE DESCRIPTION','WARD PRECINCT_most_frequent','ZIP CODE_random'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply one hot encodeing on the application type and conditional approval\n",
    "\n",
    "dataset = pd.get_dummies(dataset, drop_first=True, columns=['APPLICATION TYPE','CONDITIONAL APPROVAL'])\n",
    "testset = pd.get_dummies(testset, drop_first=True, columns=['APPLICATION TYPE','CONDITIONAL APPROVAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to encode dependen variable columns\n",
    "LICENSE_STATUS = {'AAI':0,'AAC':1,'REV':2,'REA':3,'INQ':4}\n",
    "dataset['LICENSE STATUS'] = dataset['LICENSE STATUS'].map(LICENSE_STATUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check columns from Train dataset and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainColumns = dataset.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLICATION TYPE_C_SBA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['APPLICATION TYPE_C_SBA']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_column = []\n",
    "for value in testset.columns.values:\n",
    "    if value in dataset.columns.values:\n",
    "        continue\n",
    "    else:\n",
    "        print (value)\n",
    "        missing_column.append(value)\n",
    "missing_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset.drop(missing_column, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed with Data Preprocessing\n",
    "\n",
    "## Feature Scaleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSub = dataset[(dataset['LICENSE STATUS'] == 3) | (dataset['LICENSE STATUS'] == 4) | (dataset['LICENSE STATUS'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = dataSub.drop(['ID','LICENSE ID','ACCOUNT NUMBER','LICENSE STATUS'], axis=1)\n",
    "y_sub = dataSub[['LICENSE STATUS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(X_sub.columns.values)\n",
    "col2 = list(X_sub.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "oversampler=SMOTE(kind='regular',k_neighbors=1)\n",
    "X_res, y_res = oversampler.fit_sample(X_sub, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resDF = pd.DataFrame(X_res, columns= col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resDF = pd.DataFrame(y_res, columns=['LICENSE STATUS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fin = dataset.drop(['ID','LICENSE ID','ACCOUNT NUMBER','LICENSE STATUS'], axis=1).append(X_resDF)\n",
    "y_fin = dataset[['LICENSE STATUS']].append(y_resDF)\n",
    "\n",
    "X = dataset.drop(['ID','LICENSE ID','ACCOUNT NUMBER','LICENSE STATUS'], axis=1).append(X_resDF).values\n",
    "y = dataset[['LICENSE STATUS']].append(y_resDF).values\n",
    "\n",
    "test = testset.drop(['ID','LICENSE ID','ACCOUNT NUMBER'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE('minority',random_state=101)\n",
    "X_res, y_res = oversampler.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sc.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Random Forest Classification Model to get Feature importance upto 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 300, criterion = 'entropy', random_state = 0)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select(X, y, cols, cutoff):\n",
    "    regressor = RandomForestClassifier(n_estimators = 300, random_state = 0)\n",
    "    regressor.fit(X, y)\n",
    "    global feat_imps\n",
    "    feat_imps = pd.concat([pd.DataFrame(cols, columns=['Features']),\n",
    "                       pd.DataFrame(rf.feature_importances_, columns=['Importances'])],axis=1)\n",
    "    feat_imps = feat_imps.sort_values(['Importances'], ascending=False)\n",
    "    feat_imps['Cumulative Importances'] = feat_imps['Importances'].cumsum()\n",
    "    #print(feat_imps)\n",
    "    feat_imps = feat_imps[feat_imps['Cumulative Importances'] < cutoff]\n",
    "    return feat_imps['Features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = dataset.columns[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "      <th>Cumulative Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LICENSE STATUS CHANGE DATE_NA</td>\n",
       "      <td>0.101344</td>\n",
       "      <td>0.101344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LICENSE STATUS CHANGE DATE_most_frequent_Day</td>\n",
       "      <td>0.054781</td>\n",
       "      <td>0.156126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>WARD PRECINCT_most_frequent_freq</td>\n",
       "      <td>0.052560</td>\n",
       "      <td>0.208685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PRECINCT_NA</td>\n",
       "      <td>0.050066</td>\n",
       "      <td>0.258752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LICENSE STATUS CHANGE DATE_most_frequent_Year</td>\n",
       "      <td>0.046577</td>\n",
       "      <td>0.305328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ZIP CODE_random_freq</td>\n",
       "      <td>0.042983</td>\n",
       "      <td>0.348311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LICENSE STATUS CHANGE DATE_most_frequent_Month</td>\n",
       "      <td>0.040681</td>\n",
       "      <td>0.388993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POLICE DISTRICT_NA</td>\n",
       "      <td>0.040562</td>\n",
       "      <td>0.429555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LICENSE TERM START DATE_random_Year</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.462733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PAYMENT DATE_random_Day</td>\n",
       "      <td>0.030495</td>\n",
       "      <td>0.493228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WARD PRECINCT_NA</td>\n",
       "      <td>0.030260</td>\n",
       "      <td>0.523488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LICENSE APPROVED FOR ISSUANCE_random_Day</td>\n",
       "      <td>0.028186</td>\n",
       "      <td>0.551674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CITY_freq</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.578759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WARD_NA</td>\n",
       "      <td>0.024477</td>\n",
       "      <td>0.603236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LICENSE TERM EXPIRATION DATE_random_Year</td>\n",
       "      <td>0.023869</td>\n",
       "      <td>0.627106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DATE ISSUED_Year</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.650728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LICENSE DESCRIPTION_freq</td>\n",
       "      <td>0.023371</td>\n",
       "      <td>0.674098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LICENSE TERM EXPIRATION DATE_random_Day</td>\n",
       "      <td>0.023136</td>\n",
       "      <td>0.697235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LICENSE CODE_freq</td>\n",
       "      <td>0.022449</td>\n",
       "      <td>0.719684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LICENSE TERM START DATE_random_Month</td>\n",
       "      <td>0.022194</td>\n",
       "      <td>0.741878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LICENSE APPROVED FOR ISSUANCE_random_Year</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>0.760441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LONGITUDE_NA</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.777657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PAYMENT DATE_random_Year</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>0.794387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>APPLICATION REQUIREMENTS COMPLETE_random_Year</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.810827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DATE ISSUED_Month</td>\n",
       "      <td>0.015381</td>\n",
       "      <td>0.826208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LATITUDE_NA</td>\n",
       "      <td>0.014639</td>\n",
       "      <td>0.840846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LICENSE TERM START DATE_random_Day</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.853018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LEGAL NAME_freq</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>0.864697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LICENSE APPROVED FOR ISSUANCE_random_Month</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.876295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>DOING BUSINESS AS NAME_most_frequent_freq</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>0.887555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PAYMENT DATE_random_Month</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.898504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>APPLICATION REQUIREMENTS COMPLETE_random_Day</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>0.908259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LICENSE TERM EXPIRATION DATE_random_Month</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>0.917812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ADDRESS_freq</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>0.927052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WARD_most_frequent</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.936111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LATITUDE_most_frequent</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>0.944889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Features  Importances  \\\n",
       "11                   LICENSE STATUS CHANGE DATE_NA     0.101344   \n",
       "34    LICENSE STATUS CHANGE DATE_most_frequent_Day     0.054781   \n",
       "45                WARD PRECINCT_most_frequent_freq     0.052560   \n",
       "8                                      PRECINCT_NA     0.050066   \n",
       "32   LICENSE STATUS CHANGE DATE_most_frequent_Year     0.046577   \n",
       "40                            ZIP CODE_random_freq     0.042983   \n",
       "33  LICENSE STATUS CHANGE DATE_most_frequent_Month     0.040681   \n",
       "9                               POLICE DISTRICT_NA     0.040562   \n",
       "20             LICENSE TERM START DATE_random_Year     0.033178   \n",
       "31                         PAYMENT DATE_random_Day     0.030495   \n",
       "6                                 WARD PRECINCT_NA     0.030260   \n",
       "28        LICENSE APPROVED FOR ISSUANCE_random_Day     0.028186   \n",
       "39                                       CITY_freq     0.027086   \n",
       "7                                          WARD_NA     0.024477   \n",
       "23        LICENSE TERM EXPIRATION DATE_random_Year     0.023869   \n",
       "14                                DATE ISSUED_Year     0.023622   \n",
       "44                        LICENSE DESCRIPTION_freq     0.023371   \n",
       "25         LICENSE TERM EXPIRATION DATE_random_Day     0.023136   \n",
       "42                               LICENSE CODE_freq     0.022449   \n",
       "21            LICENSE TERM START DATE_random_Month     0.022194   \n",
       "26       LICENSE APPROVED FOR ISSUANCE_random_Year     0.018562   \n",
       "13                                    LONGITUDE_NA     0.017216   \n",
       "29                        PAYMENT DATE_random_Year     0.016731   \n",
       "17   APPLICATION REQUIREMENTS COMPLETE_random_Year     0.016440   \n",
       "15                               DATE ISSUED_Month     0.015381   \n",
       "12                                     LATITUDE_NA     0.014639   \n",
       "22              LICENSE TERM START DATE_random_Day     0.012172   \n",
       "35                                 LEGAL NAME_freq     0.011679   \n",
       "27      LICENSE APPROVED FOR ISSUANCE_random_Month     0.011598   \n",
       "37       DOING BUSINESS AS NAME_most_frequent_freq     0.011260   \n",
       "30                       PAYMENT DATE_random_Month     0.010949   \n",
       "19    APPLICATION REQUIREMENTS COMPLETE_random_Day     0.009755   \n",
       "24       LICENSE TERM EXPIRATION DATE_random_Month     0.009552   \n",
       "38                                    ADDRESS_freq     0.009241   \n",
       "0                               WARD_most_frequent     0.009059   \n",
       "3                           LATITUDE_most_frequent     0.008778   \n",
       "\n",
       "    Cumulative Importances  \n",
       "11                0.101344  \n",
       "34                0.156126  \n",
       "45                0.208685  \n",
       "8                 0.258752  \n",
       "32                0.305328  \n",
       "40                0.348311  \n",
       "33                0.388993  \n",
       "9                 0.429555  \n",
       "20                0.462733  \n",
       "31                0.493228  \n",
       "6                 0.523488  \n",
       "28                0.551674  \n",
       "39                0.578759  \n",
       "7                 0.603236  \n",
       "23                0.627106  \n",
       "14                0.650728  \n",
       "44                0.674098  \n",
       "25                0.697235  \n",
       "42                0.719684  \n",
       "21                0.741878  \n",
       "26                0.760441  \n",
       "13                0.777657  \n",
       "29                0.794387  \n",
       "17                0.810827  \n",
       "15                0.826208  \n",
       "12                0.840846  \n",
       "22                0.853018  \n",
       "35                0.864697  \n",
       "27                0.876295  \n",
       "37                0.887555  \n",
       "30                0.898504  \n",
       "19                0.908259  \n",
       "24                0.917812  \n",
       "38                0.927052  \n",
       "0                 0.936111  \n",
       "3                 0.944889  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_cols = feature_select(X_res, y_res, cols, 0.95)\n",
    "feat_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_fin[imp_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85901,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y_fin['LICENSE STATUS'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE('minority',random_state=101)\n",
    "X_res, y_res = oversampler.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "meanX = sc_X.mean_\n",
    "varX = sc_X.var_\n",
    "\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model,Sequential,load_model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Activation, Embedding, LeakyReLU, BatchNormalization\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 19)                703       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 100       \n",
      "=================================================================\n",
      "Total params: 2,135\n",
      "Trainable params: 2,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(input_shape,))\n",
    "\n",
    "hidden1 = Dense(units = X_train.shape[1],kernel_initializer = 'uniform',\n",
    "                activation = 'relu')(input_layer)\n",
    "\n",
    "dropout = Dropout(rate=0.2)(hidden1)\n",
    "\n",
    "hidden2 = Dense(units = 19,kernel_initializer = 'uniform',\n",
    "                activation = 'relu')(hidden1)\n",
    "\n",
    "dropout = Dropout(rate=0.2)(hidden2)\n",
    "\n",
    "output_layer = Dense(units = 5,kernel_initializer = 'uniform',\n",
    "                activation = 'softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compiling the ANN\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['acc'])\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"Best_Model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auroc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 155812 samples, validate on 51938 samples\n",
      "Epoch 1/100\n",
      "155812/155812 [==============================] - 3s 21us/step - loss: 1.0164 - acc: 0.6512 - val_loss: 0.4704 - val_acc: 0.8397\n",
      "Epoch 2/100\n",
      " 13312/155812 [=>............................] - ETA: 1s - loss: 0.4962 - acc: 0.815"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_auroc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.3467 - acc: 0.8685 - val_loss: 0.2033 - val_acc: 0.9258\n",
      "Epoch 3/100\n",
      "155812/155812 [==============================] - 1s 10us/step - loss: 0.2151 - acc: 0.9208 - val_loss: 0.1600 - val_acc: 0.9343\n",
      "Epoch 4/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.1803 - acc: 0.9313 - val_loss: 0.1447 - val_acc: 0.9421\n",
      "Epoch 5/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.1642 - acc: 0.9376 - val_loss: 0.1348 - val_acc: 0.9455\n",
      "Epoch 6/100\n",
      "155812/155812 [==============================] - 1s 9us/step - loss: 0.1518 - acc: 0.9418 - val_loss: 0.1250 - val_acc: 0.9496\n",
      "Epoch 7/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.1410 - acc: 0.9464 - val_loss: 0.1162 - val_acc: 0.9551\n",
      "Epoch 8/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.1309 - acc: 0.9514 - val_loss: 0.1077 - val_acc: 0.9595\n",
      "Epoch 9/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.1208 - acc: 0.9559 - val_loss: 0.0987 - val_acc: 0.9635\n",
      "Epoch 10/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.1120 - acc: 0.9603 - val_loss: 0.0921 - val_acc: 0.9664\n",
      "Epoch 11/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.1052 - acc: 0.9632 - val_loss: 0.0870 - val_acc: 0.9687\n",
      "Epoch 12/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0989 - acc: 0.9657 - val_loss: 0.0842 - val_acc: 0.9698\n",
      "Epoch 13/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0912 - acc: 0.9687 - val_loss: 0.0793 - val_acc: 0.9725\n",
      "Epoch 14/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0866 - acc: 0.9708 - val_loss: 0.0759 - val_acc: 0.9740\n",
      "Epoch 15/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0829 - acc: 0.9722 - val_loss: 0.0736 - val_acc: 0.9751\n",
      "Epoch 16/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0790 - acc: 0.9737 - val_loss: 0.0705 - val_acc: 0.9765\n",
      "Epoch 17/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0758 - acc: 0.9752 - val_loss: 0.0680 - val_acc: 0.9774\n",
      "Epoch 18/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0734 - acc: 0.9760 - val_loss: 0.0658 - val_acc: 0.9781\n",
      "Epoch 19/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0715 - acc: 0.9767 - val_loss: 0.0641 - val_acc: 0.9798\n",
      "Epoch 20/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0685 - acc: 0.9779 - val_loss: 0.0644 - val_acc: 0.9789\n",
      "Epoch 21/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0668 - acc: 0.9788 - val_loss: 0.0606 - val_acc: 0.9803\n",
      "Epoch 22/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0646 - acc: 0.9798 - val_loss: 0.0597 - val_acc: 0.9808\n",
      "Epoch 23/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0634 - acc: 0.9800 - val_loss: 0.0581 - val_acc: 0.9813\n",
      "Epoch 24/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0619 - acc: 0.9805 - val_loss: 0.0572 - val_acc: 0.9816\n",
      "Epoch 25/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0604 - acc: 0.9812 - val_loss: 0.0563 - val_acc: 0.9822\n",
      "Epoch 26/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0589 - acc: 0.9819 - val_loss: 0.0539 - val_acc: 0.9833\n",
      "Epoch 27/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0578 - acc: 0.9822 - val_loss: 0.0534 - val_acc: 0.9837\n",
      "Epoch 28/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0566 - acc: 0.9826 - val_loss: 0.0526 - val_acc: 0.9841\n",
      "Epoch 29/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0555 - acc: 0.9830 - val_loss: 0.0521 - val_acc: 0.9843\n",
      "Epoch 30/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0550 - acc: 0.9831 - val_loss: 0.0517 - val_acc: 0.9844\n",
      "Epoch 31/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0538 - acc: 0.9836 - val_loss: 0.0494 - val_acc: 0.9852\n",
      "Epoch 32/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0524 - acc: 0.9841 - val_loss: 0.0492 - val_acc: 0.9853\n",
      "Epoch 33/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0512 - acc: 0.9845 - val_loss: 0.0485 - val_acc: 0.9856\n",
      "Epoch 34/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0508 - acc: 0.9846 - val_loss: 0.0474 - val_acc: 0.9859\n",
      "Epoch 35/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0498 - acc: 0.9851 - val_loss: 0.0509 - val_acc: 0.9842\n",
      "Epoch 36/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0492 - acc: 0.9852 - val_loss: 0.0465 - val_acc: 0.9862\n",
      "Epoch 37/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0486 - acc: 0.9854 - val_loss: 0.0459 - val_acc: 0.9866\n",
      "Epoch 38/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0476 - acc: 0.9858 - val_loss: 0.0450 - val_acc: 0.9869\n",
      "Epoch 39/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0471 - acc: 0.9859 - val_loss: 0.0451 - val_acc: 0.9865\n",
      "Epoch 40/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0458 - acc: 0.9865 - val_loss: 0.0436 - val_acc: 0.9871\n",
      "Epoch 41/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0456 - acc: 0.9866 - val_loss: 0.0437 - val_acc: 0.9870\n",
      "Epoch 42/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0452 - acc: 0.9869 - val_loss: 0.0434 - val_acc: 0.9873\n",
      "Epoch 43/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0442 - acc: 0.9871 - val_loss: 0.0429 - val_acc: 0.9874\n",
      "Epoch 44/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0435 - acc: 0.9870 - val_loss: 0.0419 - val_acc: 0.9881\n",
      "Epoch 45/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0435 - acc: 0.9872 - val_loss: 0.0428 - val_acc: 0.9873\n",
      "Epoch 46/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0422 - acc: 0.9877 - val_loss: 0.0405 - val_acc: 0.9883\n",
      "Epoch 47/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0418 - acc: 0.9878 - val_loss: 0.0409 - val_acc: 0.9879\n",
      "Epoch 48/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0410 - acc: 0.9882 - val_loss: 0.0399 - val_acc: 0.9889\n",
      "Epoch 49/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0397 - acc: 0.9887 - val_loss: 0.0391 - val_acc: 0.9889\n",
      "Epoch 50/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0394 - acc: 0.9885 - val_loss: 0.0402 - val_acc: 0.9883\n",
      "Epoch 51/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0400 - acc: 0.9883 - val_loss: 0.0394 - val_acc: 0.9884\n",
      "Epoch 52/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0388 - acc: 0.9886 - val_loss: 0.0378 - val_acc: 0.9890\n",
      "Epoch 53/100\n",
      "155812/155812 [==============================] - 2s 12us/step - loss: 0.0383 - acc: 0.9891 - val_loss: 0.0382 - val_acc: 0.9891\n",
      "Epoch 54/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0376 - acc: 0.9893 - val_loss: 0.0373 - val_acc: 0.9892\n",
      "Epoch 55/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0371 - acc: 0.9893 - val_loss: 0.0370 - val_acc: 0.9898\n",
      "Epoch 56/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0365 - acc: 0.9898 - val_loss: 0.0365 - val_acc: 0.9896\n",
      "Epoch 57/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0366 - acc: 0.9896 - val_loss: 0.0368 - val_acc: 0.9888\n",
      "Epoch 58/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0357 - acc: 0.9898 - val_loss: 0.0363 - val_acc: 0.9892\n",
      "Epoch 59/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0350 - acc: 0.9901 - val_loss: 0.0356 - val_acc: 0.9899\n",
      "Epoch 60/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0353 - acc: 0.9899 - val_loss: 0.0370 - val_acc: 0.9890\n",
      "Epoch 61/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0350 - acc: 0.9901 - val_loss: 0.0344 - val_acc: 0.9904\n",
      "Epoch 62/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0338 - acc: 0.9904 - val_loss: 0.0349 - val_acc: 0.9904\n",
      "Epoch 63/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0338 - acc: 0.9905 - val_loss: 0.0346 - val_acc: 0.9902\n",
      "Epoch 64/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0332 - acc: 0.9906 - val_loss: 0.0333 - val_acc: 0.9906\n",
      "Epoch 65/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0329 - acc: 0.9908 - val_loss: 0.0338 - val_acc: 0.9902\n",
      "Epoch 66/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0327 - acc: 0.9911 - val_loss: 0.0337 - val_acc: 0.9905\n",
      "Epoch 67/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0326 - acc: 0.9909 - val_loss: 0.0342 - val_acc: 0.9899\n",
      "Epoch 68/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0323 - acc: 0.9911 - val_loss: 0.0324 - val_acc: 0.9909\n",
      "Epoch 69/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0316 - acc: 0.9914 - val_loss: 0.0315 - val_acc: 0.9913\n",
      "Epoch 70/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0313 - acc: 0.9914 - val_loss: 0.0325 - val_acc: 0.9914\n",
      "Epoch 71/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0317 - acc: 0.9911 - val_loss: 0.0325 - val_acc: 0.9910\n",
      "Epoch 72/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0319 - acc: 0.9912 - val_loss: 0.0325 - val_acc: 0.9907\n",
      "Epoch 73/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0307 - acc: 0.9916 - val_loss: 0.0317 - val_acc: 0.9911\n",
      "Epoch 74/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0308 - acc: 0.9915 - val_loss: 0.0317 - val_acc: 0.9912\n",
      "Epoch 75/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0303 - acc: 0.9920 - val_loss: 0.0308 - val_acc: 0.9913\n",
      "Epoch 76/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0301 - acc: 0.9919 - val_loss: 0.0302 - val_acc: 0.9916\n",
      "Epoch 77/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0295 - acc: 0.9922 - val_loss: 0.0306 - val_acc: 0.9918\n",
      "Epoch 78/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0300 - acc: 0.9918 - val_loss: 0.0322 - val_acc: 0.9908\n",
      "Epoch 79/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.0310 - val_acc: 0.9911\n",
      "Epoch 80/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0295 - acc: 0.9920 - val_loss: 0.0295 - val_acc: 0.9919\n",
      "Epoch 81/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0285 - acc: 0.9924 - val_loss: 0.0294 - val_acc: 0.9920\n",
      "Epoch 82/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0286 - acc: 0.9923 - val_loss: 0.0298 - val_acc: 0.9917\n",
      "Epoch 83/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0283 - acc: 0.9924 - val_loss: 0.0292 - val_acc: 0.9918\n",
      "Epoch 84/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0284 - acc: 0.9925 - val_loss: 0.0297 - val_acc: 0.9916\n",
      "Epoch 85/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0281 - acc: 0.9927 - val_loss: 0.0302 - val_acc: 0.9912\n",
      "Epoch 86/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0280 - acc: 0.9925 - val_loss: 0.0291 - val_acc: 0.9919\n",
      "Epoch 87/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0280 - acc: 0.9926 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 88/100\n",
      "155812/155812 [==============================] - 1s 10us/step - loss: 0.0274 - acc: 0.9929 - val_loss: 0.0302 - val_acc: 0.9911\n",
      "Epoch 89/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0280 - acc: 0.9924 - val_loss: 0.0276 - val_acc: 0.9925\n",
      "Epoch 90/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0270 - acc: 0.9929 - val_loss: 0.0281 - val_acc: 0.9925\n",
      "Epoch 91/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0272 - acc: 0.9929 - val_loss: 0.0296 - val_acc: 0.9915\n",
      "Epoch 92/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0269 - acc: 0.9930 - val_loss: 0.0278 - val_acc: 0.9922\n",
      "Epoch 93/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0268 - acc: 0.9930 - val_loss: 0.0270 - val_acc: 0.9928\n",
      "Epoch 94/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0265 - acc: 0.9930 - val_loss: 0.0276 - val_acc: 0.9924\n",
      "Epoch 95/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0261 - acc: 0.9933 - val_loss: 0.0289 - val_acc: 0.9918\n",
      "Epoch 96/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0263 - acc: 0.9929 - val_loss: 0.0278 - val_acc: 0.9929\n",
      "Epoch 97/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0260 - acc: 0.9932 - val_loss: 0.0283 - val_acc: 0.9919\n",
      "Epoch 98/100\n",
      "155812/155812 [==============================] - 2s 11us/step - loss: 0.0259 - acc: 0.9932 - val_loss: 0.0272 - val_acc: 0.9928\n",
      "Epoch 99/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0255 - acc: 0.9933 - val_loss: 0.0275 - val_acc: 0.9923\n",
      "Epoch 100/100\n",
      "155812/155812 [==============================] - 2s 10us/step - loss: 0.0256 - acc: 0.9933 - val_loss: 0.0260 - val_acc: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22f8b580940>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "model.fit(X_train, \n",
    "          y = to_categorical(y_train),\n",
    "          verbose=1,\n",
    "          validation_split=0.25,\n",
    "          shuffle=True,\n",
    "          callbacks = callbacks_list,\n",
    "          batch_size = 1024,\n",
    "          epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3453695e-29, 7.8850053e-03, 9.9211496e-01, 0.0000000e+00,\n",
       "        1.5019857e-28],\n",
       "       [1.0000000e+00, 1.8700899e-18, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [1.0000000e+00, 3.8080801e-20, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       ...,\n",
       "       [2.7684230e-34, 9.9999988e-01, 1.2393610e-07, 0.0000000e+00,\n",
       "        3.8797492e-29],\n",
       "       [9.5928076e-17, 4.7907945e-02, 9.5209205e-01, 5.8538919e-30,\n",
       "        1.6180131e-15],\n",
       "       [1.7933169e-20, 2.8385304e-02, 9.7161466e-01, 0.0000000e+00,\n",
       "        1.8588262e-18]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936317689530686"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, precision_score, recall_score, f1_score\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "#predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "predicted = np.argmax(model.predict(X_test), axis=1)\n",
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 1, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for ANN: \n",
      " [[13935     0     0     0     0]\n",
      " [    0 13495   418     2     0]\n",
      " [    0    21 13765     0     0]\n",
      " [    0     0     0 13801     0]\n",
      " [    0     0     0     0 13813]]\n",
      "Accuracy for ANN: \n",
      " 0.9936317689530686\n",
      "Precision for ANN: \n",
      " 0.9936317689530686\n",
      "Recall for ANN: \n",
      " 0.9936317689530686\n",
      "f1_score for ANN: \n",
      " 0.9936317689530686\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for ANN: \\n',confusion_matrix(y_test, predicted))\n",
    "print('Accuracy for ANN: \\n',accuracy_score(y_test, predicted))\n",
    "acc.append(accuracy_score(y_test, predicted))\n",
    "print('Precision for ANN: \\n',precision_score(y_test, predicted, average='micro'))\n",
    "precision.append(precision_score(y_test, predicted, average='micro'))\n",
    "print('Recall for ANN: \\n',precision_score(y_test, predicted, average='micro'))\n",
    "recall.append(recall_score(y_test, predicted, average='micro'))\n",
    "print('f1_score for ANN: \\n',f1_score(y_test, predicted, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(layers, activation):\n",
    "#     model = Sequential()\n",
    "#     for i, nodes in enumerate(layers):\n",
    "#         if i==0:\n",
    "#             model.add(Dense(nodes,input_dim=X_train.shape[1]))\n",
    "#             model.add(Activation(activation))\n",
    "#         else:\n",
    "#             model.add(Dense(nodes))\n",
    "#             model.add(Activation(activation))\n",
    "#     model.add(Dense(1)) # Note: no activation beyond this point\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#     return model\n",
    "    \n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [[20], [40, 20], [45, 30, 15]]\n",
    "# activations = ['sigmoid', 'relu']\n",
    "# param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [grid_result.best_score_,grid_result.best_params_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = np.argmax(grid.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score=accuracy_score(y_test,predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
